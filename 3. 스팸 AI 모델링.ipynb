{"cells":[{"cell_type":"code","source":["# 필요 라이브러리부터 설치할께요.\n","!pip install konlpy pandas seaborn gensim wordcloud"],"metadata":{"id":"-ab09aoKXZEW"},"id":"-ab09aoKXZEW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## import sklearn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.font_manager as fm\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import nltk\n","from sklearn.linear_model import SGDClassifier\n","from konlpy.tag import Mecab\n","from sklearn.metrics import accuracy_score,f1_score,confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import MultinomialNB\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","mecab = Mecab()\n","#fm.findSystemFonts()\n","plt.rcParams['font.family']= [\"NanumGothicCoding\"]\n","plt.rcParams[\"axes.unicode_minus\"]=False\n","# GPU 환경 설정하기\n","# assert tf.test.is_gpu_available() == True, 'GPU 설정을 확인하세요.'\n","print(tf.config.list_physical_devices('GPU'))\n","print(tf.config.list_logical_devices('GPU'))"],"metadata":{"id":"N7cBVhDKXZG9"},"id":"N7cBVhDKXZG9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. 데이터 가져오기"],"metadata":{"id":"SOxu-7pWXrdm"},"id":"SOxu-7pWXrdm"},{"cell_type":"code","source":["spam = pd.read_csv('spam_1.csv')\n","# train validation set으로 분리합니다.\n","x_data = spam['text']\n","y_data = spam['label']\n","print('메일 본문의 개수: {}'.format(len(x_data)))\n","print('레이블의 개수: {}'.format(len(y_data)))\n"],"metadata":{"id":"o75VuRClXZJz"},"id":"o75VuRClXZJz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["spam_test = pd.read_csv('spam_test_text.csv')\n","# train validation set으로 분리합니다.\n","x_t = spam_test['text']\n","print('메일 본문의 개수: {}'.format(len(x_t)))\n"],"metadata":{"id":"q-qwki_DXZMW"},"id":"q-qwki_DXZMW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequence Vectorize 1\n","from tensorflow.python.keras.preprocessing import sequence\n","from tensorflow.python.keras.preprocessing import text\n","\n","TOP_K = 20000\n","MAX_SEQUENCE_LENGTH = 500\n","\n","def sequence_vectorize(test_texts):\n","    # Create vocabulary with training texts.\n","    tokenizer = text.Tokenizer(num_words=TOP_K)\n","    tokenizer.fit_on_texts(test_texts)\n","\n","    # Vectorize training and validation texts.\n","    x_train = tokenizer.texts_to_sequences(test_texts)\n","\n","    # Get max sequence length.\n","    max_length = len(max(x_train, key=len))\n","    if max_length > MAX_SEQUENCE_LENGTH:\n","        max_length = MAX_SEQUENCE_LENGTH\n","\n","    # Fix sequence length to max value. Sequences shorter than the length are\n","    # padded in the beginning and sequences longer are truncated\n","    # at the beginning.\n","    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n","    return x_train, tokenizer.word_index\n","\n","x_t2, word_to_index = sequence_vectorize(x_t)\n","print(x_t2)\n","#print(word_to_index)\n","\n","print(\"훈련 데이터의 크기(shape):\", x_t2.shape)"],"metadata":{"id":"4z0x12U0XZPH"},"id":"4z0x12U0XZPH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequence Vectorize 2 \n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_t)\n","x_train_encodedt = tokenizer.texts_to_sequences(x_t)\n","print(x_train_encodedt[:5])\n","\n","word_to_indext = tokenizer.word_index\n","#print(word_to_index)\n","\n","threshold = 2\n","total_cnt = len(word_to_index)\n","rare_cnt = 0\n","total_freq = 0\n","rare_freq = 0\n","\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq+value\n","    if value<threshold:\n","        rare_cnt=rare_cnt+1\n","        rare_freq=rare_freq+value\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n","\n","vocab_size = len(word_to_indext)+1\n","print('단어 집합의 크기: {}'.format((vocab_size)))\n","\n","print('메일의 최대 길이 : %d' % max(len(sample) for sample in x_train_encodedt))\n","print('메일의 평균 길이 : %f' % (sum(map(len, x_train_encodedt))/len(x_train_encodedt)))\n","plt.hist([len(sample) for sample in x_t], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n","\n","max_len = 23\n","x_t_padded = pad_sequences(x_train_encodedt, maxlen = max_len)\n","print(\"훈련 데이터의 크기(shape):\", x_t_padded.shape)\n"],"metadata":{"id":"lZIMUiEkXZRv"},"id":"lZIMUiEkXZRv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=0, stratify=y_data)\n","\n","print('--------훈련 데이터의 비율-----------')\n","print(f'정상 메일 = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\n","print(f'스팸 메일 = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')"],"metadata":{"id":"ACy4AO24XZUP"},"id":"ACy4AO24XZUP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequence Vectorize 1\n","from tensorflow.python.keras.preprocessing import sequence\n","from tensorflow.python.keras.preprocessing import text\n","\n","TOP_K = 20000\n","MAX_SEQUENCE_LENGTH = 500\n","\n","def sequence_vectorize(train_texts, val_texts):\n","    # Create vocabulary with training texts.\n","    tokenizer = text.Tokenizer(num_words=TOP_K)\n","    tokenizer.fit_on_texts(train_texts)\n","\n","    # Vectorize training and validation texts.\n","    x_train = tokenizer.texts_to_sequences(train_texts)\n","    x_val = tokenizer.texts_to_sequences(val_texts)\n","\n","    # Get max sequence length.\n","    max_length = len(max(x_train, key=len))\n","    if max_length > MAX_SEQUENCE_LENGTH:\n","        max_length = MAX_SEQUENCE_LENGTH\n","\n","    # Fix sequence length to max value. Sequences shorter than the length are\n","    # padded in the beginning and sequences longer are truncated\n","    # at the beginning.\n","    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n","    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n","    return x_train, x_val, tokenizer.word_index\n","\n","x_train2, x_val2, word_to_index = sequence_vectorize(x_train, x_val)\n","print(x_train2)\n","print(x_val2)\n","#print(word_to_index)\n","\n","print(\"훈련 데이터의 크기(shape):\", x_train2.shape)\n","print(\"훈련 데이터의 크기(shape):\", x_val2.shape)"],"metadata":{"id":"JVsYliwpXwkt"},"id":"JVsYliwpXwkt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequence Vectorize 2 \n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","x_train_encoded = tokenizer.texts_to_sequences(x_train)\n","print(x_train_encoded[:5])\n","\n","word_to_index = tokenizer.word_index\n","#print(word_to_index)\n","\n","threshold = 2\n","total_cnt = len(word_to_index)\n","rare_cnt = 0\n","total_freq = 0\n","rare_freq = 0\n","\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq+value\n","    if value<threshold:\n","        rare_cnt=rare_cnt+1\n","        rare_freq=rare_freq+value\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n","\n","vocab_size = len(word_to_index)+1\n","print('단어 집합의 크기: {}'.format((vocab_size)))\n","\n","print('메일의 최대 길이 : %d' % max(len(sample) for sample in x_train_encoded))\n","print('메일의 평균 길이 : %f' % (sum(map(len, x_train_encoded))/len(x_train_encoded)))\n","plt.hist([len(sample) for sample in x_data], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n","\n","max_len = 23\n","x_train_padded = pad_sequences(x_train_encoded, maxlen = max_len)\n","print(\"훈련 데이터의 크기(shape):\", x_train_padded.shape)\n"],"metadata":{"id":"carcgT8fXwnN"},"id":"carcgT8fXwnN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","def preprocess(x):\n","    porter=PorterStemmer()\n","    word_map={}\n","    cnt=0\n","    for i in range(len(x.values)):\n","        tempArr=x.values[i].split(' ')\n","        for j in range(len(tempArr)):\n","            tempArr[j]=porter.stem(tempArr[j])\n","            if tempArr[j] in word_map:\n","                continue\n","            else:\n","                word_map[tempArr[j]]=cnt\n","                cnt=cnt+1\n","        x.values[i]=np.array(tempArr)\n","    return x, word_map\n","x_data2, word_map = preprocess(x_data)\n","print(\"x_data2\", x_data2.shape)"],"metadata":{"id":"yiP22YNaXwpt"},"id":"yiP22YNaXwpt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convertToFeatureVector(x):\n","    x_len=len(x.values)\n","    word_map_len=len(word_map.keys())\n","    retArr=[]\n","    for i in range(len(x.values)):\n","        temp=np.zeros(word_map_len)\n","        for j in range(len(x.values[i])):\n","            idx = word_map.get(x.values[i][j])\n","            temp[idx]=1\n","        retArr.append(temp)\n","    return np.array(retArr)\n","x_feature_vec=convertToFeatureVector(x_data2)\n","print(x_feature_vec[10])"],"metadata":{"id":"1Q9_Czy_XwsE"},"id":"1Q9_Czy_XwsE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train3, x_test3, y_train3, y_test3 = train_test_split(x_feature_vec, y_data, stratify=y_data, test_size=0.2, random_state=2022)"],"metadata":{"id":"IOPoU5-zXwuY"},"id":"IOPoU5-zXwuY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train3[0]"],"metadata":{"id":"TE8uve0BX0bv"},"id":"TE8uve0BX0bv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Machine Learning(N-grams)\n","## 2-1. Naive Bayes"],"metadata":{"id":"hQB72kmBX2WQ"},"id":"hQB72kmBX2WQ"},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier()\n","clf.fit(x_train3, y_train3)\n","print(\"score : \", round(clf.score(x_train3, y_train3)*100,2))"],"metadata":{"id":"rjZqpnlFX2Al"},"id":"rjZqpnlFX2Al","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#print(\"test score : \", round(clf.score(x_test3, y_test3)*100,2))\n","prediction3 = clf.predict(x_t_padded)"],"metadata":{"id":"Vh1AszYLX0eA"},"id":"Vh1AszYLX0eA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Deep Learning(Sequence)\n","## 3-1. Conv1D\n","## 3-2. LSTM\n","## 3-3. RNN"],"metadata":{"id":"IRMbAYIUYAFZ"},"id":"IRMbAYIUYAFZ"},{"cell_type":"code","source":["'''\n","from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n","from tensorflow.keras.models import Sequential\n","embedding_dim = 32\n","hidden_units = 32\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(SimpleRNN(hidden_units))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(x_train3, y_train3, epochs=4, batch_size=64, validation_split=0.2)\n","\n","x_test_encoded3 = tokenizer.texts_to_sequences(x_val3)\n","x_test_padded3 = pad_sequences(x_test_encoded3, maxlen = max_len)\n","print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(x_test_padded3, y_val3)[1]))\n","'''"],"metadata":{"id":"6rwrrZV9YDLC"},"id":"6rwrrZV9YDLC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n","from tensorflow.keras.models import Sequential\n","\n","embedding_dim = 32\n","hidden_units = 32\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(SimpleRNN(hidden_units))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.fit(x_train2, y_train, epochs=4, batch_size=64, validation_split=0.2)"],"metadata":{"id":"_f6W0IPYYDlz"},"id":"_f6W0IPYYDlz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction4 = model.predict(x_t_padded)\n","prediction = model.predict(x_t)\n"],"metadata":{"id":"lfI7RRoEYDn6"},"id":"lfI7RRoEYDn6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_test_encoded2 = tokenizer.texts_to_sequences(x_val)\n","x_test_padded2 = pad_sequences(x_test_encoded2, maxlen = max_len)\n","print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(x_test_padded2, y_val)[1]))"],"metadata":{"id":"YAQ1RW0QYDqK"},"id":"YAQ1RW0QYDqK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission2 = pd.read_csv('spam_test_text.csv')\n","submission2['label']=y_predict\n","submission2['label'].unique()\n","submission2.head()\n","submission2['label'].loc[submission2['label']=='ham']\n","submission2['label'].loc[submission2['label']<0.1]\n","submission2['label'].loc[submission2['label']==0]='ham'\n","submission2['label'].loc[submission2['label']!='ham']='spam'\n","submission2.to_csv('/aihub/data/sub9.csv', index=False)\n"],"metadata":{"id":"xENRcz1oYDsR"},"id":"xENRcz1oYDsR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["----submission"],"metadata":{"id":"Iwr8S2pOYTqO"},"id":"Iwr8S2pOYTqO"},{"cell_type":"code","source":["from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","embedding_dim = 32\n","hidden_units = 32\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(SimpleRNN(hidden_units))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","es = EarlyStopping(monitor='val_loss',\n","                   min_delta=0,\n","                   patience=30,\n","                   verbose=1,\n","                   restore_best_weights=True)\n","model.fit(x_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2, callbacks=[es])"],"metadata":{"id":"xOB3DeZYYTTy"},"id":"xOB3DeZYYTTy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred2 = model.predict(X_train_padded2)\n","x_test_encoded = tokenizer.texts_to_sequences(x_val)\n","x_test_padded = pad_sequences(x_test_encoded, maxlen = max_len)\n","print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(x_test_padded, y_val)[1]))"],"metadata":{"id":"wcKFqV1uYU9r"},"id":"wcKFqV1uYU9r","execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = range(1, len(history.history['acc']) + 1)\n","plt.plot(epochs, history.history['loss'])\n","plt.plot(epochs, history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"SzYbz0U_YU_4"},"id":"SzYbz0U_YU_4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"ZcafA9mqYVCE"},"id":"ZcafA9mqYVCE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('spam_1.csv')\n","print('총 샘플의 수 :',len(data))\n","X_data = data['text']\n","Y_data = data['label']\n","print('메일 본문의 개수: {}'.format(len(x_data)))\n","print('레이블의 개수: {}'.format(len(y_data)))"],"metadata":{"id":"1OpwlAkzYXj2"},"id":"1OpwlAkzYXj2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=0, stratify=y_data)"],"metadata":{"id":"ZwNzmsqTYXmH"},"id":"ZwNzmsqTYXmH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","X_train_encoded = tokenizer.texts_to_sequences(X_train)\n","print(X_train_encoded[:5])\n","word_to_index = tokenizer.word_index\n"],"metadata":{"id":"r2ax0dOmYXom"},"id":"r2ax0dOmYXom","execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 2\n","total_cnt = len(word_to_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"],"metadata":{"id":"ROacfxMvYXq-"},"id":"ROacfxMvYXq-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(word_to_index) + 1\n","print('단어 집합의 크기: {}'.format((vocab_size)))"],"metadata":{"id":"YAaJ5oJSYa2z"},"id":"YAaJ5oJSYa2z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\n","print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\n","plt.hist([len(sample) for sample in X_data], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"metadata":{"id":"CBhkr3ZwYa4-"},"id":"CBhkr3ZwYa4-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 23\n","X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n","print(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)"],"metadata":{"id":"c9PS_ZalYa7l"},"id":"c9PS_ZalYa7l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n","from tensorflow.keras.models import Sequential\n","\n","embedding_dim = 32\n","hidden_units = 32\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(SimpleRNN(hidden_units))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(X_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2)\n","\n","X_test_encoded = tokenizer.texts_to_sequences(X_test)\n","X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n","print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))"],"metadata":{"id":"Q9WINaHOYa-C"},"id":"Q9WINaHOYa-C","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data2 = pd.read_csv('spam_test_text.csv')\n","print('총 샘플의 수 :',len(data2))\n","X_data2 = data2['text']"],"metadata":{"id":"NKA8hqbJYe-W"},"id":"NKA8hqbJYe-W","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_data2)\n","X_train_encoded2 = tokenizer.texts_to_sequences(X_data2)\n","print(X_train_encoded2[:5])\n","word_to_index2 = tokenizer.word_index\n","#print(word_to_index)"],"metadata":{"id":"Bx2JgLtBYfQJ"},"id":"Bx2JgLtBYfQJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 2\n","total_cnt = len(word_to_index2) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"],"metadata":{"id":"_N1P6lp7YfSV"},"id":"_N1P6lp7YfSV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size2 = len(word_to_index2) + 1\n","print('단어 집합의 크기: {}'.format((vocab_size2)))"],"metadata":{"id":"6Kn50O-sYfUf"},"id":"6Kn50O-sYfUf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded2))\n","print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded2))/len(X_train_encoded2)))\n","plt.hist([len(sample) for sample in X_data2], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"metadata":{"id":"U5reADDiYjwK"},"id":"U5reADDiYjwK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 22\n","X_train_padded2 = pad_sequences(X_train_encoded2, maxlen = max_len)\n","print(\"훈련 데이터의 크기(shape):\", X_train_padded2.shape)"],"metadata":{"id":"mdM6Tem2Yjyt"},"id":"mdM6Tem2Yjyt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from konlpy.tag import Mecab\n","mecab = Mecab()"],"metadata":{"id":"qrSTuazUYj1I"},"id":"qrSTuazUYj1I","execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt=' '.join(spam.text.explode())\n","nltk_spam = nltk.Text(' '.join(spam.text.explode()))\n","print(nltk_spam)"],"metadata":{"id":"aOnf0c2MYrPw"},"id":"aOnf0c2MYrPw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 태깅 클래스를 활용하여 형태소/명사를 추출합니다.\n","morphs_spam = mecab.morphs(txt)\n","print(len(morphs_spam))\n","morphs_spam[:10]"],"metadata":{"id":"oMaEj_ReYrST"},"id":"oMaEj_ReYrST","execution_count":null,"outputs":[]},{"cell_type":"code","source":["nouns_spam = mecab.nouns(txt)\n","print(len(nouns_spam))\n","nouns_spam[:10]"],"metadata":{"id":"aDHArFv0YrUz"},"id":"aDHArFv0YrUz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pos_spam = mecab.pos(txt)\n","print(len(pos_spam))\n","print(pos_spam[:3])\n","print(mecab.pos(txt,flatten=False)[:3])\n","print(mecab.pos(txt,join=True)[:3])"],"metadata":{"id":"j9WIMmFJYrXR"},"id":"j9WIMmFJYrXR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk import bigrams"],"metadata":{"id":"c0AmGGlqYt9i"},"id":"c0AmGGlqYt9i","execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk_morphs_spam = nltk.Text(morphs_spam)\n","nltk_nouns_spam = nltk.Text(nouns_spam)"],"metadata":{"id":"PaITRmTxYt_2"},"id":"PaITRmTxYt_2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"5r8P4EjIYuCS"},"id":"5r8P4EjIYuCS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv=CountVectorizer()"],"metadata":{"id":"NlqEY-NxYvyC"},"id":"NlqEY-NxYvyC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["morphs_ft=cv.fit_transform(nltk_morphs_spam)\n","morphs_ft.shape\n","!pip install beautifulsoup4\n"],"metadata":{"id":"39msVqjoYv0p"},"id":"39msVqjoYv0p","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import BernoulliNB\n","import pickle\n","import re\n","import numpy as np\n"],"metadata":{"id":"kx3vo27CYv3V"},"id":"kx3vo27CYv3V","execution_count":null,"outputs":[]},{"cell_type":"code","source":["korean_stopwords_path = \"./korean_stopwords.txt\"\n","\n","# 텍스트 파일을 오픈합니다.\n","with open(korean_stopwords_path, encoding='utf-8') as f:\n","    stopwords = f.readlines()\n","stopwords = [x.strip() for x in stopwords]"],"metadata":{"id":"iOJKOaQbYv5v"},"id":"iOJKOaQbYv5v","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_english_documents(documents):\n","    #텍스트 정제 (HTML 태그 제거)\n","    for i, document in enumerate(documents):\n","        document = BeautifulSoup(document, 'html.parser').text \n","        documents[i] = document\n","\n","    #텍스트 정제 (특수기호 제거)\n","    for i, document in enumerate(documents):\n","        document = re.sub(r'[^ A-Za-z]', '', document) #특수기호 제거, 정규 표현식 \n","        documents[i] = document\n","\n","    #텍스트 정제 (불용어 제거)\n","    nltk.download('punkt')\n","    nltk.download('stopwords')\n","    for i, document in enumerate(documents):\n","        clean_words = []\n","        for word in nltk.tokenize.word_tokenize(document):\n","            word = word.lower()\n","            if word not in nltk.corpus.stopwords.words('english'): #불용어 제거\n","                clean_words.append(word)\n","        document = ' '.join(clean_words)\n","        documents[i] = document\n","\n","    #텍스트 정제 (형태소 분석)\n","    nltk.download('averaged_perceptron_tagger')\n","    for i, document in enumerate(documents):\n","        clean_words = []\n","        for word in nltk.tag.pos_tag(nltk.tokenize.word_tokenize(document)):\n","            if word[1] in ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']: #명사, 동사, 형용사\n","                clean_words.append(word[0])\n","        document = ' '.join(clean_words)\n","        document = document.lower()\n","        documents[i] = document\n","\n","    #텍스트 정제 (어간 추출)\n","    for i, document in enumerate(documents):\n","        clean_words = []\n","        for word in nltk.tokenize.word_tokenize(document):\n","            word = word.lower()\n","            stemmer = nltk.stem.snowball.SnowballStemmer('english')\n","            word = stemmer.stem(word) #어간 추출\n","            clean_words.append(word)\n","        document = ' '.join(clean_words)\n","        documents[i] = document\n","\n","    return documents"],"metadata":{"id":"6EQTB0EHYv7_"},"id":"6EQTB0EHYv7_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["spam = pd.read_csv('spam_1.csv')\n","# train validation set으로 분리합니다.\n","x_data = spam['text']\n","y_data = spam['label']\n","print('메일 본문의 개수: {}'.format(len(x_data)))\n","print('레이블의 개수: {}'.format(len(y_data)))\n"],"metadata":{"id":"Kw2-8El1Yz09"},"id":"Kw2-8El1Yz09","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import pandas as pd\n","from tqdm import tqdm\n","from konlpy.tag import Okt\n","#from pykospacing import Spacing\n","from collections import Counter"],"metadata":{"id":"j29-aRJxYz3X"},"id":"j29-aRJxYz3X","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iP0nhFLAYz5_"},"id":"iP0nhFLAYz5_","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3o-esTJyYz8d"},"id":"3o-esTJyYz8d","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KG4ozNjqYz-0"},"id":"KG4ozNjqYz-0","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}
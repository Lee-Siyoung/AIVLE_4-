{"cells":[{"cell_type":"code","source":["## import sklearn\n","import pandas as pd\n","import matplotlib.font_manager as fm\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import nltk\n","from sklearn.linear_model import SGDClassifier\n","from konlpy.tag import Mecab\n","from sklearn.metrics import accuracy_score,f1_score,confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import MultinomialNB\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","mecab = Mecab()\n","#fm.findSystemFonts()\n","plt.rcParams['font.family']= [\"NanumGothicCoding\"]\n","plt.rcParams[\"axes.unicode_minus\"]=False\n","# GPU 환경 설정하기\n","# assert tf.test.is_gpu_available() == True, 'GPU 설정을 확인하세요.'\n","print(tf.config.list_physical_devices('GPU'))\n","print(tf.config.list_logical_devices('GPU'))"],"metadata":{"id":"z7kHekmgVtB9"},"id":"z7kHekmgVtB9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. 데이터 준비\n","## 1-1. 데이터 가져오기"],"metadata":{"id":"QF1V_1rkV1aC"},"id":"QF1V_1rkV1aC"},{"cell_type":"code","source":["# 데이터를 가져옵니다.\n","spam = pd.read_csv('spam_1.csv')\n","spam.head()\n","\n","# train validation set으로 분리합니다.\n","x_data = spam['text']\n","y_data = spam['label']\n","print('메일 본문의 개수: {}'.format(len(x_data)))\n","print('레이블의 개수: {}'.format(len(y_data)))\n","\n","x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=0, stratify=y_data)\n","\n","print('--------훈련 데이터의 비율-----------')\n","print(f'정상 메일 = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\n","print(f'스팸 메일 = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')\n","\n","print('--------테스트 데이터의 비율-----------')\n","print(f'정상 메일 = {round(y_val.value_counts()[0]/len(y_val) * 100,3)}%')\n","print(f'스팸 메일 = {round(y_val.value_counts()[1]/len(y_val) * 100,3)}%')\n","\n","y_train\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","x_train_counts = count_vect.fit_transform(x_train)\n","x_val_counts = count_vect.fit_transform(x_val)\n","print(x_train_counts.shape)\n","print(x_val_counts.shape)\n","\n","count_vect.vocabulary_.get(u'algorithm')\n","\n","from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()\n","x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n","print(x_train_tfidf.shape)\n","\n","x_val_tfidf = tfidf_transformer.fit_transform(x_val_counts)\n","print(x_val_tfidf.shape)\n","\n","from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB().fit(x_train_tfidf, x_train)"],"metadata":{"id":"4Mch9kCgVtCl"},"id":"4Mch9kCgVtCl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","\n","NGRAM_RANGE = (1, 2)\n","TOP_K = 20000\n","TOKEN_MODE = 'word'\n","MIN_DOCUMENT_FREQUENCY = 2\n","def ngram_vectorize(train_texts, train_labels, val_texts):\n","    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n","    kwargs = {\n","            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n","            'dtype': 'int32',\n","            'strip_accents': 'unicode',\n","            'decode_error': 'replace',\n","            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n","            'min_df': MIN_DOCUMENT_FREQUENCY,\n","    }\n","    vectorizer = TfidfVectorizer(**kwargs)\n","\n","    # Learn vocabulary from training texts and vectorize training texts.\n","    x_train = vectorizer.fit_transform(train_texts)\n","\n","    # Vectorize validation texts.\n","    x_val = vectorizer.transform(val_texts)\n","\n","    # Select top 'k' of the vectorized features.\n","    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n","    selector.fit(x_train, train_labels)\n","    x_train = selector.transform(x_train).astype('float32')\n","    x_val = selector.transform(x_val).astype('float32')\n","    return x_train, x_val\n","\n","x_train1 , x_val1= ngram_vectorize(x_train, y_train, x_val)\n","\n","print(x_train1)\n","print(x_val1)\n","#print(word_to_index)\n","\n","print(\"훈련 데이터의 크기(shape):\", x_train1.shape)\n","print(\"훈련 데이터의 크기(shape):\", x_val1.shape)\n","'''"],"metadata":{"id":"p75P0V8FVtDL"},"id":"p75P0V8FVtDL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.python.keras.preprocessing import sequence\n","from tensorflow.python.keras.preprocessing import text\n","\n","TOP_K = 20000\n","MAX_SEQUENCE_LENGTH = 500\n","\n","def sequence_vectorize(train_texts, val_texts):\n","    # Create vocabulary with training texts.\n","    tokenizer = text.Tokenizer(num_words=TOP_K)\n","    tokenizer.fit_on_texts(train_texts)\n","\n","    # Vectorize training and validation texts.\n","    x_train = tokenizer.texts_to_sequences(train_texts)\n","    x_val = tokenizer.texts_to_sequences(val_texts)\n","\n","    # Get max sequence length.\n","    max_length = len(max(x_train, key=len))\n","    if max_length > MAX_SEQUENCE_LENGTH:\n","        max_length = MAX_SEQUENCE_LENGTH\n","\n","    # Fix sequence length to max value. Sequences shorter than the length are\n","    # padded in the beginning and sequences longer are truncated\n","    # at the beginning.\n","    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n","    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n","    return x_train, x_val, tokenizer.word_index\n","\n","x_train2, x_val2, word_to_index = sequence_vectorize(x_train, x_val)\n","\n","print(x_train2)\n","print(x_val2)\n","#print(word_to_index)\n","\n","print(\"훈련 데이터의 크기(shape):\", x_train2.shape)\n","print(\"훈련 데이터의 크기(shape):\", x_val2.shape)\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","x_train_encoded = tokenizer.texts_to_sequences(x_train)\n","print(x_train_encoded[:5])\n","\n","word_to_index = tokenizer.word_index\n","#print(word_to_index)\n","\n","threshold = 2\n","total_cnt = len(word_to_index)\n","rare_cnt = 0\n","total_freq = 0\n","rare_freq = 0\n","\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq+value\n","    if value<threshold:\n","        rare_cnt=rare_cnt+1\n","        rare_freq=rare_freq+value\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n","\n","vocab_size = len(word_to_index)+1\n","print('단어 집합의 크기: {}'.format((vocab_size)))\n","\n","print('메일의 최대 길이 : %d' % max(len(sample) for sample in x_train_encoded))\n","print('메일의 평균 길이 : %f' % (sum(map(len, x_train_encoded))/len(x_train_encoded)))\n","plt.hist([len(sample) for sample in x_data], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n","\n","max_len = 23\n","x_train_padded = pad_sequences(x_train_encoded, maxlen = max_len)\n","print(\"훈련 데이터의 크기(shape):\", x_train_padded.shape)\n","\n","print(x_val2)\n","\n","import konlpy.tag"],"metadata":{"id":"03zIB8cPVtDt"},"id":"03zIB8cPVtDt","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XJ0BkY-eVtES"},"id":"XJ0BkY-eVtES","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ypRee0jyVtE3"},"id":"ypRee0jyVtE3","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}